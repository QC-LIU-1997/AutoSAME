{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np \n",
    "import cv2\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.models.resnet import resnet34\n",
    "from torchvision.models.video import r3d_18\n",
    "from torchsummary import summary\n",
    "from torchstat import stat\n",
    "from math import floor\n",
    "import cmath\n",
    "from scipy.special import expit\n",
    "import math\n",
    "from scipy.ndimage import label\n",
    "import random\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=3407\n",
    "random.seed(seed)   # Python的随机性\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)    # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "np.random.seed(seed)   # numpy的随机性\n",
    "torch.manual_seed(seed)   # torch的CPU随机性，为CPU设置随机种子\n",
    "torch.cuda.manual_seed(seed)   # torch的GPU随机性，为当前GPU设置随机种子\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "torch.backends.cudnn.benchmark = False   # if benchmark=True, deterministic will be False\n",
    "torch.backends.cudnn.deterministic = True   # 选择确定性算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils.config import get_config\n",
    "from models.model_dict import get_model\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Networks')\n",
    "parser.add_argument('--modelname', default='SAME', type=str, help='type of model, e.g., SAM, SAMFull, MedSAM, MSA, SAMed, SAMUS...')\n",
    "parser.add_argument('-encoder_input_size', type=int, default=256, help='the image size of the encoder input, 1024 in SAM and MSA, 512 in SAMed, 256 in SAMUS')\n",
    "parser.add_argument('-low_image_size', type=int, default=128, help='the image embedding size, 256 in SAM and MSA, 128 in SAMed and SAMUS')\n",
    "parser.add_argument('--task', default='US30K', help='task or dataset name')\n",
    "parser.add_argument('--vit_name', type=str, default='vit_b', help='select the vit model for the image encoder of sam')\n",
    "parser.add_argument('--sam_ckpt', type=str, default='sam_vit_b_01ec64.pth', help='Pretrained checkpoint of SAM')\n",
    "parser.add_argument('--fcba_flag', type=bool, default=None, help='FCBA')\n",
    "parser.add_argument('--sgpa_flag', type=bool, default=None, help='AGPA')\n",
    "parser.add_argument('--bott_flag', type=bool, default=None, help='BOTT')\n",
    "parser.add_argument(\"-f\", dest = 'j_cfile', help = \"jupyter config file\", default = \"file.json\", type = str)\n",
    "args = parser.parse_known_args()[0]\n",
    "opt = get_config(args.task) \n",
    "args.bott_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fcba_flag = False\n",
    "args.sgpa_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0002\n",
    "NFrame = 10\n",
    "epochs_fin = 60\n",
    "epochs_wm = 10\n",
    "epochs_ot = epochs_fin-epochs_wm\n",
    "lambda1 = lambda epoch: (epoch / epochs_wm) if epoch < epochs_wm else 0.5 * (math.cos((epoch - epochs_wm)/(epochs_ot) * math.pi) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamusIterator(Dataset):\n",
    "    def __init__(self,data_list,do_aug=True):\n",
    "\n",
    "        self.data_list = data_list\n",
    "        self.do_aug=do_aug\n",
    "\n",
    "    def __read_image_gt(self,pat_i,label):\n",
    "        ED_LV_gt_file = 'D:/CAMUS_240307/patient{}_{}CH_ED_LV_gt.png'.format(pat_i,label)\n",
    "        ES_LV_gt_file = 'D:/CAMUS_240307/patient{}_{}CH_ES_LV_gt.png'.format(pat_i,label)\n",
    "        ED_LV_gt = cv2.imread(ED_LV_gt_file,0)\n",
    "        ES_LV_gt = cv2.imread(ES_LV_gt_file,0)\n",
    "        return ED_LV_gt,ES_LV_gt\n",
    "\n",
    "    def __read_cfg(self,pat_i,label):\n",
    "        cfg_str = 'D:/CAMUS_240307/patient{}_Info_{}CH.cfg'.format(pat_i,label)\n",
    "  \n",
    "        with open(cfg_str, 'r') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "        try:\n",
    "\n",
    "            lv_edv_match = re.search(r'LVedv:\\s*([\\d.]+)', file_content)\n",
    "            EDV=float(lv_edv_match.group(1))\n",
    "\n",
    "\n",
    "            lv_esv_match = re.search(r'LVesv:\\s*([\\d.]+)', file_content)\n",
    "            ESV = float(lv_esv_match.group(1))\n",
    "\n",
    "            lv_ef_match = re.search(r'LVef:\\s*([\\d.]+)', file_content)\n",
    "            EF_para = float(lv_ef_match.group(1))\n",
    "\n",
    "            w_match = re.search(r'w:\\s*([\\d.]+)', file_content)\n",
    "            W = float(w_match.group(1))\n",
    "            h_match = re.search(r'h:\\s*([\\d.]+)', file_content)\n",
    "            H = float(h_match.group(1))\n",
    "            im_size = np.array((H,W))\n",
    "            EV_para = np.around(np.array((EDV, ESV)))\n",
    "\n",
    "            return im_size,EV_para,EF_para\n",
    "        except:\n",
    "            EDV = ESV = 0\n",
    "            EF_para = 0\n",
    "            w_match = re.search(r'w:\\s*([\\d.]+)', file_content)\n",
    "            W = float(w_match.group(1))\n",
    "            h_match = re.search(r'h:\\s*([\\d.]+)', file_content)\n",
    "            H = float(h_match.group(1))\n",
    "            im_size = np.array((H,W))\n",
    "            EV_para = np.around(np.array((EDV, ESV)))\n",
    "            return im_size,EV_para,EF_para\n",
    "\n",
    "        return im_size,EV_para,EF_para\n",
    "    \n",
    "    def __read_seq( self,pat_i,label):\n",
    "        sequence_str = 'D:/CAMUS_240307/patient{}_{}CH_sequence.npy'.format(pat_i,label)\n",
    "        sequence = np.load(sequence_str)   \n",
    "        return sequence\n",
    "\n",
    "    def __read_keypoint(self,pat_i,label):\n",
    "        ED_key_point_str = \"D:/CAMUS_240307/patient{}_{}CH_ED_5_Points.npy\".format(pat_i,label)\n",
    "        ES_key_point_str = \"D:/CAMUS_240307/patient{}_{}CH_ES_5_Points.npy\".format(pat_i,label)\n",
    "        ED_key_point=np.load(ED_key_point_str)[0:3,:]\n",
    "        ES_key_point = np.load(ES_key_point_str)[0:3,:]\n",
    "        return ED_key_point,ES_key_point\n",
    "    \n",
    "    def __len__( self ):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__( self, index):\n",
    "        ED_LV_gt_first,ES_LV_gt_first = self.__read_image_gt(self.data_list[index],2)\n",
    "        ED_key_point_first,ES_key_point_first = self.__read_keypoint(self.data_list[index],2)\n",
    "        im_size_first,EV_para_first,EF_para_first = self.__read_cfg(self.data_list[index],2)\n",
    "        sequence_first = self.__read_seq(self.data_list[index],2)\n",
    "\n",
    "\n",
    "        ED_LV_gt_last,ES_LV_gt_last = self.__read_image_gt(self.data_list[index],4)\n",
    "        ED_key_point_last,ES_key_point_last = self.__read_keypoint(self.data_list[index],4)\n",
    "        im_size_last,EV_para_last,EF_para_last = self.__read_cfg(self.data_list[index],4)\n",
    "        sequence_last = self.__read_seq(self.data_list[index],4)\n",
    "\n",
    "        im_size = np.array([im_size_first,im_size_last])\n",
    "\n",
    "\n",
    "        ED_LV_gt = torch.tensor(np.array((ED_LV_gt_first,ED_LV_gt_last)), dtype = torch.uint8)\n",
    "        ES_LV_gt = torch.tensor(np.array((ES_LV_gt_first,ES_LV_gt_last)), dtype = torch.uint8)\n",
    "        ED_key_point = torch.tensor(np.array((ED_key_point_first,ED_key_point_last)), dtype = torch.uint8)\n",
    "        ES_key_point = torch.tensor(np.array((ES_key_point_first,ES_key_point_last)), dtype = torch.uint8)\n",
    "        tensor_sequence = torch.tensor(np.array((sequence_first,sequence_last)), dtype = torch.float32)\n",
    "        tensor_EV_para = torch.tensor(np.array((EV_para_first,EV_para_last)), dtype = torch.float32)\n",
    "        tensor_EF_para = torch.tensor(np.array((EF_para_first,EF_para_last)), dtype = torch.float32)\n",
    " \n",
    "        data = {\n",
    "            'index': index,\n",
    "            'sequence': tensor_sequence,\n",
    "            'size': im_size,\n",
    "            'ED_LV_gt': ED_LV_gt,\n",
    "            'ES_LV_gt': ES_LV_gt,\n",
    "            'ED_key_point':ED_key_point,\n",
    "            'ES_key_point':ES_key_point,\n",
    "            'EV_para': tensor_EV_para,\n",
    "            'EF_para': tensor_EF_para\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('CAMUS_split_fin.xlsx')\n",
    "\n",
    "train_index_list = [format(i, \"04\") for i in df[df['fold'] == 'Train']['pat_i']]\n",
    "test_index_list = [format(i, \"04\") for i in df[df['fold'] == 'Test']['pat_i']]\n",
    "\n",
    "train_valid_iter = CamusIterator(data_list = train_index_list,do_aug=True)\n",
    "test_iter = CamusIterator(data_list = test_index_list,do_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(n, val_pct, seed):\n",
    "    # Determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "    # Set the random seed (for reproducibility)\n",
    "    np.random.seed(seed)\n",
    "    # Create random permutation of 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "\n",
    "val_pct = 1/9\n",
    "train_indices, val_indices = split_indices(len(train_valid_iter), val_pct, seed)\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "traindata = DataLoader(train_valid_iter, sampler=train_sampler, batch_size = 1)\n",
    "valdata = DataLoader(train_valid_iter, sampler=val_sampler, batch_size = 1)\n",
    "testdata = DataLoader(test_iter, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_point_2_onehot(gt_image,key_point):\n",
    "    key_point_y = key_point[:,0]\n",
    "    key_point_x = key_point[:,1]\n",
    "    onehot = np.zeros((3,gt_image.shape[-2],gt_image.shape[-1]))\n",
    "    for k in range(onehot.shape[0]):\n",
    "        onehot[k,(key_point_y[k]-2):(key_point_y[k]+2),(key_point_x[k]-2):(key_point_x[k]+2)]= 0.5\n",
    "        onehot[k,(key_point_y[k]-1):(key_point_y[k]+1),(key_point_x[k]-1):(key_point_x[k]+1)]= 0.75\n",
    "        onehot[k][key_point_y[k]][key_point_x[k]]= 1\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def key_point_2_heatmap(gt_image,key_point,mask_r,ratio):\n",
    "    mask_r = mask_r//ratio\n",
    "    key_point_y = key_point[:,0]//ratio\n",
    "    key_point_x = key_point[:,1]//ratio\n",
    "    heat_map = np.zeros((3,gt_image.shape[-2]//ratio,gt_image.shape[-1]//ratio))\n",
    "    for k in range(heat_map.shape[0]):\n",
    "        for i in range(heat_map.shape[1]):\n",
    "            for j in range(heat_map.shape[2]):\n",
    "                distance_2 = (i-key_point_y[k])**2+(j-key_point_x[k])**2\n",
    "                heat_map[k][i][j]= np.e ** (-1 * (distance_2  / (2 * mask_r ** 2)))\n",
    "    # heat_map = np.sum(heat_map,axis = 0)\n",
    "    return heat_map.flatten()\n",
    "\n",
    "def heatmap_2_key_point(heatmap):\n",
    "    key_point = np.zeros((3,2))\n",
    "    for idx in range(3):\n",
    "        img = heatmap[idx]\n",
    "        M = img.argmax()\n",
    "        key_point[idx][0] = M//img.shape[1]\n",
    "        key_point[idx][1] = M%img.shape[1]\n",
    "    return key_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_aug_fin(aug_flag,ED_LV_gt_raw,ES_LV_gt_raw,ED_key_point_raw,ES_key_point_raw,sequence_raw,mask_r):\n",
    "    ED_LV_gt_new = []\n",
    "    ES_LV_gt_new = []\n",
    "    ED_LV_heatmap_new = []\n",
    "    ES_LV_heatmap_new = []\n",
    "    ED_LV_point_new = []\n",
    "    ES_LV_point_new = []\n",
    "    sequence_new = []\n",
    "\n",
    "    for batch_i in range(len(sequence_raw)):\n",
    "        ED_LV_gt_batch = []\n",
    "        ES_LV_gt_batch = []\n",
    "        ED_LV_heatmap_batch = []\n",
    "        ES_LV_heatmap_batch = []\n",
    "        sequence_batch = []\n",
    "        ED_LV_point_batch = []\n",
    "        ES_LV_point_batch = []\n",
    "\n",
    "        for paired_i in range(len(sequence_raw[batch_i])):\n",
    "            sequence_list = []\n",
    "            if aug_flag == True:\n",
    "                ED_heatmap_raw = key_point_2_onehot(ED_LV_gt_raw[batch_i][paired_i],ED_key_point_raw[batch_i][paired_i])\n",
    "                ES_heatmap_raw = key_point_2_onehot(ES_LV_gt_raw[batch_i][paired_i],ES_key_point_raw[batch_i][paired_i])\n",
    "                theta = random.uniform(-15,15)\n",
    "                scale = random.uniform(0.95,1.05)\n",
    "                ratio = random.uniform(0.95,1.05)\n",
    "                distortion_scale = random.uniform(0,0.05)\n",
    "                prob = random.randint(0,1)\n",
    "\n",
    "                \n",
    "                geo_transforms = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation((theta,theta),expand=True),transforms.CenterCrop(256),\n",
    "                                                     transforms.RandomPerspective(distortion_scale=0.3,p=prob),transforms.RandomResizedCrop((256,256),scale=(scale,scale),ratio = (ratio,ratio))])\n",
    "\n",
    "\n",
    "                concat_image = np.concatenate((np.expand_dims(ED_LV_gt_raw[batch_i][paired_i],axis=0),\n",
    "                                               np.expand_dims(ES_LV_gt_raw[batch_i][paired_i],axis=0),\n",
    "                                               ED_heatmap_raw,ES_heatmap_raw,\n",
    "                                               sequence_raw[batch_i][paired_i])).transpose(1,2,0)\n",
    "                concat_transform = geo_transforms(concat_image)\n",
    "\n",
    "\n",
    "                ED_LV_gt_temp = concat_transform[0,:,:].numpy()\n",
    "                ES_LV_gt_temp = concat_transform[1,:,:].numpy()\n",
    "                \n",
    "                ED_LV_heatmap_repack = concat_transform[2:5,:,:].numpy()\n",
    "                ES_LV_heatmap_repack = concat_transform[5:8,:,:].numpy()\n",
    "\n",
    "                sequence_temp= concat_transform[8:10,:,:].numpy()\n",
    "\n",
    "                ED_key_point_num_temp = heatmap_2_key_point(ED_LV_heatmap_repack)\n",
    "                ES_key_point_num_temp = heatmap_2_key_point(ES_LV_heatmap_repack)\n",
    "                ED_LV_point_batch.append(ED_key_point_num_temp)\n",
    "                ES_LV_point_batch.append(ES_key_point_num_temp)\n",
    "\n",
    "                ED_LV_heatmap_temp = key_point_2_heatmap(ED_LV_gt_temp,ED_key_point_num_temp,mask_r,2)\n",
    "                ES_LV_heatmap_temp = key_point_2_heatmap(ES_LV_gt_temp,ES_key_point_num_temp,mask_r,2)\n",
    "            else:\n",
    "                ED_LV_gt_temp = ED_LV_gt_raw[batch_i][paired_i]\n",
    "                ES_LV_gt_temp = ES_LV_gt_raw[batch_i][paired_i]\n",
    "                ED_LV_heatmap_temp = key_point_2_heatmap(ED_LV_gt_raw[batch_i][paired_i],ED_key_point_raw[batch_i][paired_i],mask_r,2)\n",
    "                ES_LV_heatmap_temp = key_point_2_heatmap(ES_LV_gt_raw[batch_i][paired_i],ES_key_point_raw[batch_i][paired_i],mask_r,2)\n",
    "                for frame_i in range(len(sequence_raw[batch_i][paired_i])):\n",
    "                    sequence_list.append(sequence_raw[batch_i][paired_i][frame_i])\n",
    "                sequence_temp= np.array(sequence_list)\n",
    "                ED_LV_point_batch.append(ED_key_point_raw[batch_i][paired_i])\n",
    "                ES_LV_point_batch.append(ES_key_point_raw[batch_i][paired_i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ED_LV_gt_temp = (ED_LV_gt_temp>((np.min(ED_LV_gt_temp)+np.max(ED_LV_gt_temp))/2)).astype(int)\n",
    "            ES_LV_gt_temp = (ES_LV_gt_temp>((np.min(ES_LV_gt_temp)+np.max(ES_LV_gt_temp))/2)).astype(int)\n",
    "\n",
    "            ED_LV_gt_batch.append(ED_LV_gt_temp)\n",
    "            ES_LV_gt_batch.append(ES_LV_gt_temp)\n",
    "            ED_LV_heatmap_batch.append(ED_LV_heatmap_temp)\n",
    "            ES_LV_heatmap_batch.append(ES_LV_heatmap_temp)\n",
    "            sequence_batch.append(sequence_temp)\n",
    "\n",
    "        ED_LV_gt_new.append(ED_LV_gt_batch)\n",
    "        ES_LV_gt_new.append(ES_LV_gt_batch)\n",
    "        ED_LV_heatmap_new.append(ED_LV_heatmap_batch)\n",
    "        ES_LV_heatmap_new.append(ES_LV_heatmap_batch)\n",
    "        ED_LV_point_new.append(ED_LV_point_batch)\n",
    "        ES_LV_point_new.append(ES_LV_point_batch)\n",
    "        sequence_new.append(sequence_batch)\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(ED_LV_gt_new),np.array(ES_LV_gt_new),np.array(ED_LV_heatmap_new),np.array(ES_LV_heatmap_new),np.array(ED_LV_point_new),np.array(ES_LV_point_new),np.array(sequence_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epoch_end(outputs):\n",
    "    batch_losses = [x['loss'] for x in outputs]\n",
    "    Seg_losses = [x['Seg_loss'] for x in outputs]\n",
    "    HR_losses = [x['HR_loss'] for x in outputs]\n",
    "    EB_losses = [x['EB_loss'] for x in outputs]\n",
    "\n",
    "    epoch_loss = torch.stack(batch_losses).mean()\n",
    "    epoch_Seg_loss = np.mean(Seg_losses)\n",
    "    epoch_HR_loss = np.mean(HR_losses)\n",
    "    epoch_EB_loss = np.mean(EB_losses)\n",
    "\n",
    "    return {'epoch_loss':epoch_loss.item(),'epoch_Seg_loss':epoch_Seg_loss,'epoch_HR_loss':epoch_HR_loss,'epoch_EB_loss':epoch_EB_loss}\n",
    "def result_epoch_end(epoch,epochs, train_result):\n",
    "    print(\"Epoch [{}/{}], Train loss: {:.4f}, Seg loss: {:.4f}, HR loss: {:.4f}, EB loss: {:.4f} \".format(epoch+1,epochs, train_result['epoch_loss'], train_result['epoch_Seg_loss'], train_result['epoch_HR_loss'], train_result['epoch_EB_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chMask(image):\n",
    "    arr = np.array(image)\n",
    "    # print('Hi: ',arr.shape)\n",
    "    new_arr = np.zeros((arr.shape[0],arr.shape[1],arr.shape[2]))\n",
    "    dice_arr = np.zeros((arr.shape[0],2,arr.shape[1],arr.shape[2]))\n",
    "    # print(arr.shape)\n",
    "    # print(new_arr.shape)\n",
    "    # print('elements: ',np.unique(arr))\n",
    "    ele = np.unique(arr)\n",
    "    for i in range(arr.shape[0]):\n",
    "        for y in range(arr.shape[2]):\n",
    "            for x in range(arr.shape[1]):\n",
    "                if arr[i, x,y] == ele[0]:\n",
    "                    new_arr[i,x,y] = 0\n",
    "                    dice_arr[i,0,x,y] = 1\n",
    "                \n",
    "                elif arr[i, x,y] == ele[1]:\n",
    "                    new_arr[i,x,y] = 1\n",
    "                    dice_arr[i,1,x,y] = 1\n",
    "                \n",
    "    #new_arr for cross entropy: N,H,W   and  dice_arr for dice loss: N,C,H,W \n",
    "    return new_arr,dice_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    # print(target.shape,pred.shape)\n",
    "    target = torch.tensor(target)\n",
    "    smooth = 0.0001\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.contiguous().view(num, -1)  # Flatten\n",
    "    m1 = m1.to(device)\n",
    "    # print(type(m1))\n",
    "    m2 = target.contiguous().view(num, -1)  # Flatten\n",
    "    m2 = m2.to(device)\n",
    "    # print(type(m2))\n",
    "    intersection = (m1 * m2).sum()\n",
    "    return 1-(2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(pred, target):\n",
    "    smooth = 0.0001\n",
    "\n",
    "    # print(pred.shape,target.shape)\n",
    "    \n",
    "    pred_LV = pred[:,1]\n",
    "    target_LV = target[:,1]\n",
    "    intersection = (pred_LV * target_LV).sum()\n",
    "\n",
    " \n",
    "    return (2. * intersection + smooth) / (pred_LV.sum() + target_LV.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCK(true_HR_list,pred_HR_list):\n",
    "    CorrectKeypoint=0\n",
    "    for i in range(len(true_HR_list)):\n",
    "        true_HR = true_HR_list[i]\n",
    "        pred_HR = pred_HR_list[i]\n",
    "        dis = ((true_HR[0]-pred_HR[0])**2+(true_HR[1]-pred_HR[1])**2)**(1/2)\n",
    "        for j in range(len(dis)):\n",
    "            if dis[j]<=12.8:\n",
    "                CorrectKeypoint=CorrectKeypoint+1\n",
    "\n",
    "        TotalKeypoint=len(true_HR_list)*3\n",
    "        Pck=CorrectKeypoint/TotalKeypoint\n",
    "\n",
    "    return Pck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_click(mask_list,point_num=4, class_id=1):\n",
    "    pt_list = []\n",
    "    pt_label_list = []\n",
    "    for mask in mask_list:\n",
    "        indices = np.argwhere(mask == class_id)\n",
    "        indices[:, [0,1]] = indices[:, [1,0]]\n",
    "        point_label = class_id\n",
    "        pt = indices[np.random.randint(len(indices),size=point_num)]\n",
    "        pt_list.append(pt)\n",
    "        pt_label_list.append([point_label]*4)\n",
    "    return np.array(pt_list), np.array(pt_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_clicks(mask_list, class_id=1, pos_prompt_number=2, neg_prompt_number=2):\n",
    "    pt_list = []\n",
    "    pt_label_list = []\n",
    "    for mask in mask_list:\n",
    "        pos_indices = np.argwhere(mask == class_id)\n",
    "        pos_indices[:, [0,1]] = pos_indices[:, [1,0]]\n",
    "        pos_prompt_indices = np.random.randint(len(pos_indices), size=pos_prompt_number)\n",
    "        pos_prompt = pos_indices[pos_prompt_indices]\n",
    "        pos_label = np.repeat(1, pos_prompt_number)\n",
    "\n",
    "        neg_indices = np.argwhere(mask != class_id)\n",
    "        neg_indices[:, [0,1]] = neg_indices[:, [1,0]]\n",
    "        neg_prompt_indices = np.random.randint(len(neg_indices), size=neg_prompt_number)\n",
    "        neg_prompt = neg_indices[neg_prompt_indices]\n",
    "        neg_label = np.repeat(0, neg_prompt_number)\n",
    "\n",
    "        pt = np.vstack((pos_prompt, neg_prompt))\n",
    "        point_label = np.hstack((pos_label, neg_label))\n",
    "        pt_list.append(pt)\n",
    "        pt_label_list.append(point_label)\n",
    "    return np.array(pt_list), np.array(pt_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anatomic_click(mask_list,key_point_list, class_id=1):\n",
    "    pt_list = []\n",
    "    pt_label_list = []\n",
    "    assert len(key_point_list) == len(mask_list)\n",
    "    for b_i in range(len(key_point_list)):\n",
    "        point_label = class_id\n",
    "        key_points = key_point_list[b_i].copy()\n",
    "        key_points[:,[0,1]] =  key_points[:,[1,0]]\n",
    "        # print('kp',key_points)\n",
    "        mask = mask_list[b_i]\n",
    "\n",
    "        indices = np.argwhere(mask == class_id)\n",
    "        indices[:, [0,1]] = indices[:, [1,0]]\n",
    "        pt = list(key_points)\n",
    "        for pi in pt:\n",
    "            if pi not in indices:\n",
    "                arg_i = np.argmin(np.sum(np.abs(indices-pi),axis = 1))\n",
    "                pi = indices[arg_i]\n",
    "                re_f = True\n",
    "                out_p = [pt,indices,mask]\n",
    "        pt_list.append(pt)\n",
    "        pt_label_list.append([point_label]*3)\n",
    "    return np.array(pt_list), np.array(pt_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_bbox(mask_list, class_id = 1, img_size=256):\n",
    "    bbox_list = []\n",
    "    for mask in mask_list:\n",
    "        indices = np.argwhere(mask == class_id) # Y X (0, 1)\n",
    "        indices[:, [0,1]] = indices[:, [1,0]]\n",
    "        if indices.shape[0] ==0:\n",
    "            return np.array([-1, -1, img_size, img_size])\n",
    "        minx = np.min(indices[:, 0])\n",
    "        maxx = np.max(indices[:, 0])\n",
    "        miny = np.min(indices[:, 1])\n",
    "        maxy = np.max(indices[:, 1])\n",
    "        bbox_list.append(np.array([minx, miny, maxx, maxy]))\n",
    "    return np.array(bbox_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_EV_model_fin(model,NFrame,train_dl,val_dl,epochs,epochs_wm,opt,scheduler,sgpa_flag):\n",
    "    MSE_loss_func = nn.MSELoss(reduction='mean')\n",
    "    cosine_loss_func = nn.CosineEmbeddingLoss()\n",
    "    history = []\n",
    "    wins_len = 3\n",
    "    valid_wins_loss = []\n",
    "    best_epoch = 0\n",
    "    best_model_params = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 99999.9\n",
    "    for epoch in range(epochs):\n",
    "        alpha = 100\n",
    "        beta = 2000\n",
    "        \n",
    "        train_stats = []\n",
    "        val_stats = []\n",
    "        temp_gamma_flag = 1.0 - 1.0*epoch/epochs_wm\n",
    "        gamma_flag = np.clip(temp_gamma_flag,0,1)\n",
    "     \n",
    "        if sgpa_flag == True:\n",
    "            gamma = 100*gamma_flag\n",
    "        else:          \n",
    "            gamma = 0\n",
    "\n",
    "        if epoch<=epochs_wm:\n",
    "            temp_fuse_apg = 1.0*epoch/epochs_wm\n",
    "            mask_r = 20\n",
    "        else:\n",
    "            temp_fuse_apg = 1.0\n",
    "            if mask_r>10:\n",
    "                mask_r = mask_r-0.25\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        for train_dl_i in tqdm(train_dl):\n",
    "            temp_ED_LV_gt = train_dl_i['ED_LV_gt'].numpy()\n",
    "            temp_ES_LV_gt = train_dl_i['ES_LV_gt'].numpy()\n",
    "            temp_ED_key_point = train_dl_i['ED_key_point'].numpy()\n",
    "            temp_ES_key_point = train_dl_i['ES_key_point'].numpy()\n",
    "            train_sequence = train_dl_i['sequence'].numpy()[:,:,[0,-1],:,:]\n",
    "            im_size = train_dl_i['size'].squeeze(0).numpy()\n",
    "\n",
    "            temp_ED_LV_gt,temp_ES_LV_gt,temp_ED_LV_heatmap,temp_ES_LV_heatmap,temp_ED_LV_point,temp_ES_LV_point,train_sequence = geo_aug_fin(True,temp_ED_LV_gt,temp_ES_LV_gt,temp_ED_key_point,temp_ES_key_point,train_sequence,mask_r)\n",
    "            ED_LV_gt = np.squeeze(temp_ED_LV_gt)\n",
    "            ES_LV_gt = np.squeeze(temp_ES_LV_gt)\n",
    "            ED_LV_point = np.squeeze(temp_ED_LV_point)\n",
    "            ES_LV_point = np.squeeze(temp_ES_LV_point)\n",
    "            # print(ED_LV_gt.shape)\n",
    "\n",
    "            _, ED_dice_mask = chMask(ED_LV_gt)\n",
    "            _, ES_dice_mask = chMask(ES_LV_gt)\n",
    "\n",
    "\n",
    "            ED_LV_heatmap = torch.tensor(np.squeeze(temp_ED_LV_heatmap), dtype = torch.float32).to(device)\n",
    "            ES_LV_heatmap = torch.tensor(np.squeeze(temp_ES_LV_heatmap), dtype = torch.float32).to(device)\n",
    "\n",
    "\n",
    "            temp_EDV_image = torch.tensor(train_sequence[:,:,0,:,:], dtype = torch.float32).squeeze(0).unsqueeze(1).to(device)\n",
    "            temp_ESV_image = torch.tensor(train_sequence[:,:,-1,:,:], dtype = torch.float32).squeeze(0).unsqueeze(1).to(device)\n",
    "            ED_pt,ED_pt_label = anatomic_click(ED_LV_gt,ED_LV_point, class_id=1)\n",
    "            ES_pt,ES_pt_label = anatomic_click(ES_LV_gt,ES_LV_point, class_id=1)\n",
    "            ED_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "            ES_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "\n",
    "            ED_pt = torch.tensor(ED_pt).to(device)\n",
    "            ED_pt_label = torch.tensor(ED_pt_label).to(device)\n",
    "            ES_pt = torch.tensor(ES_pt).to(device)\n",
    "            ES_pt_label = torch.tensor(ES_pt_label).to(device)\n",
    "            ED_bbox = torch.tensor(ED_bbox).to(device)\n",
    "            ES_bbox = torch.tensor(ES_bbox).to(device)\n",
    "\n",
    "\n",
    "            EDV_model_out = model(temp_EDV_image,(ED_pt,ED_pt_label), ED_bbox, fuse_apg=temp_fuse_apg)\n",
    "            ESV_model_out = model(temp_ESV_image,(ES_pt,ES_pt_label), ES_bbox, fuse_apg=temp_fuse_apg)\n",
    "\n",
    "            Seg_EDV_output =  EDV_model_out['seg_masks'].sigmoid()\n",
    "            Seg_ESV_output =  ESV_model_out['seg_masks'].sigmoid()\n",
    "            Seg_EDV_output = torch.concat([(1-Seg_EDV_output),Seg_EDV_output],dim=1)\n",
    "            Seg_ESV_output = torch.concat([(1-Seg_ESV_output),Seg_ESV_output],dim=1)\n",
    "\n",
    "            HR_EDV_output = EDV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_ESV_output = ESV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_EDV_output = HR_EDV_output.reshape(HR_EDV_output.shape[0],-1)\n",
    "            HR_ESV_output = HR_ESV_output.reshape(HR_ESV_output.shape[0],-1)\n",
    "\n",
    "            Seg_EDV_loss = dice_loss(Seg_EDV_output,ED_dice_mask)       \n",
    "            Seg_ESV_loss = dice_loss(Seg_ESV_output,ES_dice_mask)\n",
    "\n",
    "            \n",
    "            HR_EDV_loss = MSE_loss_func(HR_EDV_output,ED_LV_heatmap)\n",
    "            HR_ESV_loss = MSE_loss_func(HR_ESV_output,ES_LV_heatmap)\n",
    "\n",
    "\n",
    "            Seg_ED_B,Seg_ED_C,Seg_ED_N = EDV_model_out['seg_se'].shape\n",
    "            Seg_ED_se = EDV_model_out['seg_se'].reshape(Seg_ED_B*Seg_ED_C,Seg_ED_N)\n",
    "            HR_ED_B,HR_ED_C,HR_ED_N = EDV_model_out['hr_se'].shape\n",
    "            HR_ED_se = EDV_model_out['hr_se'].reshape(HR_ED_B*HR_ED_C,HR_ED_N)\n",
    "\n",
    "            Seg_EDV_se_apg =  EDV_model_out['seg_se_apg'].reshape(Seg_ED_B*Seg_ED_C,Seg_ED_N)\n",
    "            HR_EDV_se_apg =  EDV_model_out['hr_se_apg'].reshape(HR_ED_B*HR_ED_C,HR_ED_N)       \n",
    "\n",
    "            Seg_ES_B,Seg_ES_C,Seg_ES_N = ESV_model_out['seg_se'].shape\n",
    "            Seg_ES_se = ESV_model_out['seg_se'].reshape(Seg_ES_B*Seg_ES_C,Seg_ES_N)\n",
    "            HR_ES_B,HR_ES_C,HR_ES_N = ESV_model_out['hr_se'].shape\n",
    "            HR_ES_se = ESV_model_out['hr_se'].reshape(HR_ES_B*HR_ES_C,HR_ES_N)\n",
    "\n",
    "            Seg_ESV_se_apg =  ESV_model_out['seg_se_apg'].reshape(Seg_ES_B*Seg_ES_C,Seg_ES_N)\n",
    "            HR_ESV_se_apg =  ESV_model_out['hr_se_apg'].reshape(HR_ES_B*HR_ES_C,HR_ES_N)\n",
    "            \n",
    "\n",
    "            ED_se_loss = cosine_loss_func(Seg_EDV_se_apg,Seg_ED_se,torch.ones(Seg_ED_B*Seg_ED_C).to(device))+cosine_loss_func(HR_EDV_se_apg,HR_ED_se,torch.ones(HR_ED_B*HR_ED_C).to(device))\n",
    "            ES_se_loss = cosine_loss_func(Seg_ESV_se_apg,Seg_ES_se,torch.ones(Seg_ES_B*Seg_ES_C).to(device))+cosine_loss_func(HR_ESV_se_apg,HR_ES_se,torch.ones(HR_ES_B*HR_ES_C).to(device))\n",
    "\n",
    "\n",
    "            loss = alpha*(Seg_EDV_loss +Seg_ESV_loss)+beta*(HR_EDV_loss+HR_ESV_loss)+gamma*(ED_se_loss + ES_se_loss)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_stats.append([loss.item(),alpha*(Seg_EDV_loss +Seg_ESV_loss).item(),beta*(HR_EDV_loss+HR_ESV_loss).item(),gamma*(ED_se_loss + ES_se_loss).item()])\n",
    "\n",
    "            \n",
    "            del train_dl_i,EDV_model_out,ESV_model_out,Seg_EDV_output,Seg_ESV_output,HR_EDV_output,HR_ESV_output,Seg_EDV_se_apg,HR_EDV_se_apg,Seg_ESV_se_apg,HR_ESV_se_apg\n",
    "            del loss,Seg_EDV_loss,Seg_ESV_loss,HR_EDV_loss,HR_ESV_loss,ED_se_loss,ES_se_loss\n",
    "\n",
    "            # gc.collect()\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "        for val_dl_i in tqdm(val_dl):\n",
    "            temp_ED_LV_gt = val_dl_i['ED_LV_gt'].numpy()\n",
    "            temp_ES_LV_gt = val_dl_i['ES_LV_gt'].numpy()\n",
    "            temp_ED_key_point = val_dl_i['ED_key_point'].numpy()\n",
    "            temp_ES_key_point = val_dl_i['ES_key_point'].numpy()\n",
    "            val_sequence = val_dl_i['sequence'].numpy()[:,:,[0,-1],:,:]\n",
    "            im_size = val_dl_i['size'].squeeze(0).numpy()\n",
    "\n",
    "            temp_ED_LV_gt,temp_ES_LV_gt,temp_ED_LV_heatmap,temp_ES_LV_heatmap,temp_ED_LV_point,temp_ES_LV_point,val_sequence = geo_aug_fin(False,temp_ED_LV_gt,temp_ES_LV_gt,temp_ED_key_point,temp_ES_key_point,val_sequence,mask_r)\n",
    "\n",
    "            # geo_para =np.squeeze(val_dl_i['geo_para'].squeeze(0).numpy()).T\n",
    "            # geo_sequence_para = np.round(np.linspace(geo_para[0,:], geo_para[-1,:], num=NFrame, endpoint=True))\n",
    "\n",
    "            ED_LV_gt = np.squeeze(temp_ED_LV_gt)\n",
    "            ES_LV_gt = np.squeeze(temp_ES_LV_gt)\n",
    "            ED_LV_point = np.squeeze(temp_ED_LV_point)\n",
    "            ES_LV_point = np.squeeze(temp_ES_LV_point)\n",
    "\n",
    "            _, ED_dice_mask = chMask(ED_LV_gt)\n",
    "            _, ES_dice_mask = chMask(ES_LV_gt)\n",
    "\n",
    "\n",
    "            ED_LV_heatmap = torch.tensor(np.squeeze(temp_ED_LV_heatmap), dtype = torch.float32).to(device)\n",
    "            ES_LV_heatmap = torch.tensor(np.squeeze(temp_ES_LV_heatmap), dtype = torch.float32).to(device)\n",
    "\n",
    "\n",
    "            temp_EDV_image = torch.tensor(val_sequence[:,:,0,:,:], dtype = torch.float32).squeeze(0).unsqueeze(1).to(device)\n",
    "            temp_ESV_image = torch.tensor(val_sequence[:,:,-1,:,:], dtype = torch.float32).squeeze(0).unsqueeze(1).to(device)\n",
    "            ED_pt,ED_pt_label = anatomic_click(ED_LV_gt, ED_LV_point, class_id=1)\n",
    "            ES_pt,ES_pt_label = anatomic_click(ES_LV_gt, ES_LV_point, class_id=1)\n",
    "            ED_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "            ES_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "\n",
    "            ED_pt = torch.tensor(ED_pt).to(device)\n",
    "            ED_pt_label = torch.tensor(ED_pt_label).to(device)\n",
    "            ES_pt = torch.tensor(ES_pt).to(device)\n",
    "            ES_pt_label = torch.tensor(ES_pt_label).to(device)\n",
    "            ED_bbox = torch.tensor(ED_bbox).to(device)\n",
    "            ES_bbox = torch.tensor(ES_bbox).to(device)\n",
    "\n",
    "            EDV_model_out = model(temp_EDV_image,(ED_pt,ED_pt_label), ED_bbox, fuse_apg=temp_fuse_apg)\n",
    "            ESV_model_out = model(temp_ESV_image,(ES_pt,ES_pt_label), ES_bbox, fuse_apg=temp_fuse_apg)\n",
    "\n",
    "            Seg_EDV_output =  EDV_model_out['seg_masks'].sigmoid()\n",
    "            Seg_ESV_output =  ESV_model_out['seg_masks'].sigmoid()\n",
    "            Seg_EDV_output = torch.concat([(1-Seg_EDV_output),Seg_EDV_output],dim=1)\n",
    "            Seg_ESV_output = torch.concat([(1-Seg_ESV_output),Seg_ESV_output],dim=1)\n",
    "\n",
    "            HR_EDV_output = EDV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_ESV_output = ESV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_EDV_output = HR_EDV_output.reshape(HR_EDV_output.shape[0],-1)\n",
    "            HR_ESV_output = HR_ESV_output.reshape(HR_ESV_output.shape[0],-1)\n",
    "\n",
    "            Seg_EDV_loss = dice_loss(Seg_EDV_output,ED_dice_mask).item() \n",
    "            Seg_ESV_loss = dice_loss(Seg_ESV_output,ES_dice_mask).item()   \n",
    "\n",
    "            \n",
    "            HR_EDV_loss = MSE_loss_func(HR_EDV_output,ED_LV_heatmap).item()   \n",
    "            HR_ESV_loss = MSE_loss_func(HR_ESV_output,ES_LV_heatmap).item()\n",
    "\n",
    "            Seg_ED_B,Seg_ED_C,Seg_ED_N = EDV_model_out['seg_se'].shape\n",
    "            Seg_ED_se = EDV_model_out['seg_se'].reshape(Seg_ED_B*Seg_ED_C,Seg_ED_N)\n",
    "            HR_ED_B,HR_ED_C,HR_ED_N = EDV_model_out['hr_se'].shape\n",
    "            HR_ED_se = EDV_model_out['hr_se'].reshape(HR_ED_B*HR_ED_C,HR_ED_N)\n",
    "\n",
    "            Seg_EDV_se_apg =  EDV_model_out['seg_se_apg'].reshape(Seg_ED_B*Seg_ED_C,Seg_ED_N)\n",
    "            HR_EDV_se_apg =  EDV_model_out['hr_se_apg'].reshape(HR_ED_B*HR_ED_C,HR_ED_N)       \n",
    "\n",
    "            Seg_ES_B,Seg_ES_C,Seg_ES_N = ESV_model_out['seg_se'].shape\n",
    "            Seg_ES_se = ESV_model_out['seg_se'].reshape(Seg_ES_B*Seg_ES_C,Seg_ES_N)\n",
    "            HR_ES_B,HR_ES_C,HR_ES_N = ESV_model_out['hr_se'].shape\n",
    "            HR_ES_se = ESV_model_out['hr_se'].reshape(HR_ES_B*HR_ES_C,HR_ES_N)\n",
    "\n",
    "            Seg_ESV_se_apg =  ESV_model_out['seg_se_apg'].reshape(Seg_ES_B*Seg_ES_C,Seg_ES_N)\n",
    "            HR_ESV_se_apg =  ESV_model_out['hr_se_apg'].reshape(HR_ES_B*HR_ES_C,HR_ES_N)\n",
    "            \n",
    "\n",
    "            ED_se_loss = cosine_loss_func(Seg_EDV_se_apg,Seg_ED_se,torch.ones(Seg_ED_B*Seg_ED_C).to(device)).item()+cosine_loss_func(HR_EDV_se_apg,HR_ED_se,torch.ones(HR_ED_B*HR_ED_C).to(device)).item()\n",
    "            ES_se_loss = cosine_loss_func(Seg_ESV_se_apg,Seg_ES_se,torch.ones(Seg_ES_B*Seg_ES_C).to(device)).item()+cosine_loss_func(HR_ESV_se_apg,HR_ES_se,torch.ones(HR_ES_B*HR_ES_C).to(device)).item()\n",
    "\n",
    "            loss = alpha*(Seg_EDV_loss +Seg_ESV_loss)+beta*(HR_EDV_loss+HR_ESV_loss)+gamma*(ED_se_loss+ES_se_loss)\n",
    "\n",
    "            val_stats.append([loss,alpha*(Seg_EDV_loss +Seg_ESV_loss),beta*(HR_EDV_loss+HR_ESV_loss),gamma*(ED_se_loss+ES_se_loss)])\n",
    "\n",
    "            del val_dl_i,EDV_model_out,ESV_model_out,Seg_EDV_output,Seg_ESV_output,HR_EDV_output,HR_ESV_output,Seg_EDV_se_apg,HR_EDV_se_apg,Seg_ESV_se_apg,HR_ESV_se_apg\n",
    "            del loss,Seg_EDV_loss,Seg_ESV_loss,HR_EDV_loss,HR_ESV_loss,ED_se_loss,ES_se_loss\n",
    "            # gc.collect()\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        print('Epoch: ',epoch, 'Train Loss:', '%.4f' % np.mean(np.array(train_stats)[:,0]),'%.4f' %  np.mean(np.array(train_stats)[:,1]),'%.4f' % np.mean(np.mean(np.array(train_stats)[:,2])),\n",
    "            '%.4f' % np.mean(np.array(train_stats)[:,3]),'Valid Loss:', '%.4f' % np.mean(np.array(val_stats)[:,0]))\n",
    "        if np.mean(np.array(val_stats)[:,0])<best_loss:\n",
    "            best_epoch = epoch+1\n",
    "            best_model_params = copy.deepcopy(model.state_dict())\n",
    "            best_loss = np.mean(np.array(val_stats)[:,0])\n",
    "\n",
    "    model.load_state_dict(best_model_params)\n",
    "    print('Best Validation Loss:{:.4f} at Epoch:{}'.format(best_loss,best_epoch))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EV_model_fin =  get_model(args.modelname, args=args, opt=opt)\n",
    "# EV_model_fin = EV_model_fin.to(device)\n",
    "# EV_opt_fin = torch.optim.Adam(EV_model_fin.parameters(), lr=lr)\n",
    "# scheduler =  torch.optim.lr_scheduler.LambdaLR(EV_opt_fin, lr_lambda=lambda1)\n",
    "# EV_model_fin,EV_history_fin = train_EV_model_fin(EV_model_fin,NFrame,traindata,valdata,epochs_fin,epochs_wm,EV_opt_fin,scheduler,args.sgpa_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_2_key_point(heatmap,temp_seg,ratio):\n",
    "    raw_key_point = np.zeros((3,2))\n",
    "\n",
    "    for idx in range(3):\n",
    "        img = cv2.resize(heatmap[idx],(temp_seg.shape[0],temp_seg.shape[1]))\n",
    "        img = img*temp_seg\n",
    "        M = img.argmax()\n",
    "        raw_key_point[idx][0] = M//img.shape[1]\n",
    "        raw_key_point[idx][1] = M%img.shape[1]\n",
    "    return raw_key_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HR_output_2_key_point(key_point_true,HR_output,temp_seg,ratio=2):\n",
    "    H = W = round(np.sqrt(HR_output.shape[1]//3))\n",
    "    dis_list = []\n",
    "    kp_list = []\n",
    "    for paired_i in range(HR_output.shape[0]):\n",
    "        key_point_output = heatmap_2_key_point(np.reshape(HR_output[paired_i],(3,H,W)),temp_seg[paired_i],ratio)\n",
    "        # print(key_point_output.shape)\n",
    "        # print(key_point_true[paired_i].shape)\n",
    "        dis = np.sqrt(np.sum(np.square(key_point_output - key_point_true[paired_i]),axis=1))\n",
    "        dis_list.append(dis)\n",
    "        kp_list.append(key_point_output)\n",
    "    dis_array = np.array(dis_list)\n",
    "    kp_array = np.array(kp_list)\n",
    "    \n",
    "    \n",
    "    return dis_array, kp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_dropout(m):\n",
    "    if type(m) == nn.Dropout:\n",
    "        m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_2_key_point(heatmap,temp_seg,ratio):\n",
    "    raw_key_point = np.zeros((3,2))\n",
    "    img_plt = np.zeros_like(temp_seg)\n",
    "\n",
    "    for idx in range(3):\n",
    "        \n",
    "        img = cv2.resize(heatmap[idx],(temp_seg.shape[0],temp_seg.shape[1]))\n",
    "        M = img.argmax()\n",
    "        raw_key_point[idx][0] = M//img.shape[1]\n",
    "        raw_key_point[idx][1] = M%img.shape[1]\n",
    "\n",
    "    return raw_key_point\n",
    "\n",
    "def HR_output_2_key_point(key_point_true,HR_output,temp_seg,ratio=2):\n",
    "    H = W = round(np.sqrt(HR_output.shape[1]//3))\n",
    "    # print('H, W: ',H,W)\n",
    "    dis_list = []\n",
    "    kp_list = []\n",
    "    for paired_i in range(HR_output.shape[0]):\n",
    "        key_point_output = heatmap_2_key_point(np.reshape(HR_output[paired_i],(3,H,W)),temp_seg[paired_i],ratio)\n",
    "        dis = np.sqrt(np.sum(np.square(key_point_output - key_point_true[paired_i]),axis=1))\n",
    "        dis_list.append(dis)\n",
    "        kp_list.append(key_point_output)\n",
    "    dis_array = np.array(dis_list)\n",
    "    kp_array = np.array(kp_list)\n",
    "    \n",
    "    \n",
    "    return dis_array, kp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing(batch_im):\n",
    "    batch = batch_im.shape[0]\n",
    "    batch_im_new = np.zeros_like(batch_im)\n",
    "    for image_i in range(batch):\n",
    "        image = batch_im[image_i]\n",
    "\n",
    "        for_which_classes = np.unique(image)\n",
    "        for_which_classes = for_which_classes[for_which_classes > 0]\n",
    "\n",
    "        assert 0 not in for_which_classes, \"cannot remove background\"\n",
    "        largest_removed = {}\n",
    "        kept_size = {}\n",
    "        for c in for_which_classes:\n",
    "            if isinstance(c, (list, tuple)):\n",
    "                c = tuple(c)  # otherwise it cant be used as key in the dict\n",
    "                mask = np.zeros_like(image, dtype=bool)\n",
    "                for cl in c:\n",
    "                    mask[image == cl] = True\n",
    "            else:\n",
    "                mask = image == c\n",
    "            # get labelmap and number of objects\n",
    "            lmap, num_objects = label(mask.astype(int))\n",
    "\n",
    "            # collect object sizes\n",
    "            object_sizes = {}\n",
    "            for object_id in range(1, num_objects + 1):\n",
    "                object_sizes[object_id] = (lmap == object_id).sum() \n",
    "\n",
    "            largest_removed[c] = None\n",
    "            kept_size[c] = None\n",
    "\n",
    "            if num_objects > 0:\n",
    "                # we always keep the largest object. We could also consider removing the largest object if it is smaller\n",
    "                # than minimum_valid_object_size in the future but we don't do that now.\n",
    "                maximum_size = max(object_sizes.values())\n",
    "                kept_size[c] = maximum_size\n",
    "\n",
    "                for object_id in range(1, num_objects + 1):\n",
    "                    # we only remove objects that are not the largest\n",
    "                    if object_sizes[object_id] != maximum_size:\n",
    "                        # we only remove objects that are smaller than minimum_valid_object_size\n",
    "                        remove = True\n",
    "                        if remove:\n",
    "                            image[(lmap == object_id) & mask] = 0\n",
    "                            if largest_removed[c] is None:\n",
    "                                largest_removed[c] = object_sizes[object_id]\n",
    "                            else:\n",
    "                                largest_removed[c] = max(largest_removed[c], object_sizes[object_id])\n",
    "        \n",
    "        image = image.astype(int)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        floodfill_mask = np.zeros((h+2, w+2), np.uint8)\n",
    "        isbreak = False\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in range(image.shape[1]):\n",
    "                if(image[i][j]==0):\n",
    "                    seedPoint=(i,j)\n",
    "                    isbreak = True\n",
    "                    break\n",
    "            if(isbreak):\n",
    "                break\n",
    "        image = image.astype(np.uint8).copy()\n",
    "        cv2.floodFill(image, floodfill_mask,seedPoint, 255)\n",
    "        # 孔洞填充函数\n",
    "        img_floodfill_raw = cv2.bitwise_not(image)\n",
    "        batch_im_new[image_i] = img_floodfill_raw\n",
    "\n",
    "    return batch_im_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_EV_model_fin(model,test_dl):\n",
    "    test_Seg_EDV_list = []\n",
    "    test_Seg_ESV_list = []\n",
    "    test_HR_EDV_list = []\n",
    "    test_HR_ESV_list = []\n",
    "    gt_para_list = []\n",
    "    pred_para_list = []\n",
    "    model.apply(close_dropout)\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for test_dl_i in tqdm(test_dl):\n",
    "            idx = idx+1\n",
    "            temp_ED_LV_gt = test_dl_i['ED_LV_gt'].squeeze(0).numpy()\n",
    "            temp_ES_LV_gt = test_dl_i['ES_LV_gt'].squeeze(0).numpy()\n",
    "            temp_ED_key_point = test_dl_i['ED_key_point'].squeeze(0).numpy()\n",
    "            temp_ES_key_point = test_dl_i['ES_key_point'].squeeze(0).numpy()\n",
    "            test_sequence= test_dl_i['sequence'].squeeze(0).numpy()[:,[0,-1],:,:]\n",
    "            im_size = test_dl_i['size'].squeeze(0).numpy()\n",
    "\n",
    "            temp_EF_para = test_dl_i['EF_para']\n",
    "            temp_EV_para = test_dl_i['EV_para']\n",
    "\n",
    "            ED_LV_gt = temp_ED_LV_gt\n",
    "            ES_LV_gt = temp_ES_LV_gt\n",
    "            \n",
    "            _, ED_dice_mask = chMask(ED_LV_gt)\n",
    "            _, ES_dice_mask = chMask(ES_LV_gt)\n",
    "\n",
    "            temp_EDV_image = torch.tensor(test_sequence[:,0,:,:], dtype = torch.float32).unsqueeze(1)\n",
    "            temp_ESV_image = torch.tensor(test_sequence[:,-1,:,:], dtype = torch.float32).unsqueeze(1)\n",
    "\n",
    "            ED_pt,ED_pt_label = anatomic_click(ED_LV_gt, temp_ED_key_point, class_id=1)\n",
    "            ES_pt,ES_pt_label = anatomic_click(ES_LV_gt, temp_ES_key_point, class_id=1)\n",
    "            ED_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "            ES_bbox = fixed_bbox(ED_LV_gt, class_id=1)\n",
    "\n",
    "            ED_pt = torch.tensor(ED_pt).to(device)\n",
    "            ED_pt_label = torch.tensor(ED_pt_label).to(device)\n",
    "            ES_pt = torch.tensor(ES_pt).to(device)\n",
    "            ES_pt_label = torch.tensor(ES_pt_label).to(device)\n",
    "            ED_bbox = torch.tensor(ED_bbox).to(device)\n",
    "            ES_bbox = torch.tensor(ES_bbox).to(device)\n",
    "            temp_EDV_image = temp_EDV_image.to(device)\n",
    "            temp_ESV_image = temp_ESV_image.to(device)\n",
    "            ED_pt = torch.tensor(ED_pt).to(device)\n",
    "            ED_pt_label = torch.tensor(ED_pt_label).to(device)\n",
    "            ES_pt = torch.tensor(ES_pt).to(device)\n",
    "            ES_pt_label = torch.tensor(ES_pt_label).to(device)\n",
    "            EDV_model_out = model(temp_EDV_image,(ED_pt,ED_pt_label), ED_bbox, fuse_apg=1.0)\n",
    "            ESV_model_out = model(temp_ESV_image,(ES_pt,ES_pt_label), ES_bbox, fuse_apg=1.0)\n",
    "            Seg_EDV_output =  EDV_model_out['seg_masks'].sigmoid()>0.5\n",
    "            Seg_ESV_output =  ESV_model_out['seg_masks'].sigmoid()>0.5\n",
    "\n",
    "            HR_EDV_output = EDV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_ESV_output = ESV_model_out['hr_low_res'].sigmoid()\n",
    "            HR_EDV_output = HR_EDV_output.reshape(HR_EDV_output.shape[0],-1)\n",
    "            HR_ESV_output = HR_ESV_output.reshape(HR_ESV_output.shape[0],-1)\n",
    "\n",
    "\n",
    "            # EDV_im = torch.argmax(Seg_EDV_output,dim=1)\n",
    "            Seg_EDV_output_np = Seg_EDV_output.squeeze().cpu().numpy().astype(float)\n",
    "            Seg_EDV_output_np = post_processing(Seg_EDV_output_np)\n",
    "            _, Seg_EDV_output_dice = chMask(Seg_EDV_output_np)\n",
    "            # ESV_im = torch.argmax(Seg_ESV_output,dim=1)\n",
    "            Seg_ESV_output_np = Seg_ESV_output.squeeze().cpu().numpy().astype(float)\n",
    "            Seg_ESV_output_np = post_processing(Seg_ESV_output_np)\n",
    "            _, Seg_ESV_output_dice = chMask(Seg_ESV_output_np)\n",
    "\n",
    "            test_Seg_EDV_list.append([dice_coeff(Seg_EDV_output_dice,ED_dice_mask)])\n",
    "            test_Seg_ESV_list.append([dice_coeff(Seg_ESV_output_dice,ES_dice_mask)])\n",
    "\n",
    "            EDV_key_point_dis,EDV_key_point_pre = HR_output_2_key_point(temp_ED_key_point,HR_EDV_output.squeeze().cpu().numpy(),ratio=2,temp_seg = Seg_EDV_output_np)\n",
    "            ESV_key_point_dis,ESV_key_point_pre = HR_output_2_key_point(temp_ES_key_point,HR_ESV_output.squeeze().cpu().numpy(),ratio=2,temp_seg = Seg_ESV_output_np)\n",
    "            test_HR_EDV_list.append(EDV_key_point_dis)\n",
    "            test_HR_ESV_list.append(ESV_key_point_dis)\n",
    "\n",
    "        df_Seg_para=pd.DataFrame([[np.mean((np.mean(np.array(test_Seg_EDV_list)[:,0]),np.mean(np.array(test_Seg_ESV_list)[:,0])))]],\n",
    "                                   columns=['Dice'])\n",
    "        df_HR_para=pd.DataFrame([[np.mean((np.mean(np.array(test_HR_EDV_list)<12.8),np.mean(np.array(test_HR_ESV_list)<12.8)))]],\n",
    "                                     columns=['PCK'])\n",
    "    return df_Seg_para,df_HR_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Seg_para,df_HR_para = test_EV_model_fin(EV_model_fin,testdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
